# ğŸ§  Logistic Regression: ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent) ì´í•´í•˜ê¸°

## ğŸš€ ì†Œê°œ

ì´ì „ ê¸€ì—ì„œ **ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)** ì˜ ë¹„ìš© í•¨ìˆ˜(Cost Function)ë¥¼ ì •ì˜í•˜ê³ , ì†ì‹¤ í•¨ìˆ˜(Loss Function)ì™€ì˜ ì°¨ì´ë¥¼ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” **ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)** ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

## ğŸ“Œ ê²½ì‚¬ í•˜ê°•ë²•ì´ë€?

ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì€ ë¹„ìš© í•¨ìˆ˜ J(w, b)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ìµœì ì˜ **ê°€ì¤‘ì¹˜(w)ì™€ ë°”ì´ì–´ìŠ¤(b)** ë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.<br/>
![Gradient Descent](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/gradient-descent-convex-function.png)

![Gradient Descent](https://mlpills.dev/wp-content/uploads/2022/10/CaIB7lz-h.jpg)

### ğŸ“Š ë¹„ìš© í•¨ìˆ˜ì™€ ìµœì í™” ëª©í‘œ

ë¹„ìš© í•¨ìˆ˜ $J(w, b)$ ëŠ” **ëª¨ë“  í›ˆë ¨ ìƒ˜í”Œì—ì„œ ì†ì‹¤ í•¨ìˆ˜ì˜ í‰ê· ** ì…ë‹ˆë‹¤.  
$(J(w, b) = - \\frac{1}{m} \\sum\_{i=1}^{m} \\big(y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)})\\big))$

ì´ ë¹„ìš© í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ **ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµì˜ í•µì‹¬ ëª©í‘œ** ì…ë‹ˆë‹¤.

---

## ğŸ¯ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ì§ê´€ì  ì´í•´

ë¹„ìš© í•¨ìˆ˜ $J(w, b)$ ëŠ” íŒŒë¼ë¯¸í„°í„°($w$, $b$)ì— ë”°ë¼ ë³€í™”í•˜ëŠ” **ê³¡ë©´ í˜•íƒœì˜ í•¨ìˆ˜**ì…ë‹ˆë‹¤.

-   ë¹„ìš© í•¨ìˆ˜ì˜ **ìµœì†Ÿê°’**ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.
-   ìµœì†Ÿê°’ì„ ì°¾ê¸° ìœ„í•´, **ê°€ì¥ ê°€íŒŒë¥¸ ë‚´ë¦¬ë§‰ ë°©í–¥ìœ¼ë¡œ ì´ë™**í•´ì•¼ í•©ë‹ˆë‹¤.
-   ì´ë¥¼ ìœ„í•´ ë¹„ìš© í•¨ìˆ˜ì˜ **ë¯¸ë¶„(Gradient, ê¸°ìš¸ê¸°)** ì„ ê³„ì‚°í•˜ì—¬ **ê°€ì¥ ë¹ ë¥´ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥**ì„ ì°¾ìŠµë‹ˆë‹¤.

### ğŸ“Œ ë¹„ìš© í•¨ìˆ˜ì˜ ê·¸ë˜í”„ ì˜ˆì‹œ

```
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def cost_function(w, b):
    return np.log(1 + np.exp(-(w * 3 + b)))  # ì˜ˆì œ ë¹„ìš© í•¨ìˆ˜

w_values = np.linspace(-5, 5, 100)
b_values = np.linspace(-5, 5, 100)
W, B = np.meshgrid(w_values, b_values)
J = cost_function(W, B)

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(W, B, J, cmap='viridis')
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_zlabel('J(w, b)')
ax.set_title('Cost Function J(w, b)')
plt.show()
```

---

## ğŸ”¢ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìˆ˜ì‹

ê²½ì‚¬ í•˜ê°•ë²•ì€ ì•„ë˜ì˜ ìˆ˜ì‹ì„ ë°˜ë³µí•˜ì—¬ ê°€ì¤‘ì¹˜($w$)ì™€ ë°”ì´ì–´ìŠ¤($b$)ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

### ğŸ“Œ ê²½ì‚¬ í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜

#### **ê°€ì¤‘ì¹˜($w$) ì—…ë°ì´íŠ¸**

$(w := w - \\alpha \\frac{\\partial J}{\\partial w})$

#### **ë°”ì´ì–´ìŠ¤($b$) ì—…ë°ì´íŠ¸**

$(b := b - \\alpha \\frac{\\partial J}{\\partial b})$

ì—¬ê¸°ì„œ,

-   alpha (alpha): **í•™ìŠµë¥ (Learning Rate)**, í•œ ë²ˆì— ì´ë™í•˜ëŠ” í¬ê¸°
-   âˆ‚J/âˆ‚w: $w$ì— ëŒ€í•œ **ë¹„ìš© í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°**
-   âˆ‚J/âˆ‚b: $b$ì— ëŒ€í•œ **ë¹„ìš© í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°**

---

## ğŸ”¥ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ë™ì‘ ë°©ì‹

1.  **ì´ˆê¸°í™”**: $w$ì™€ $b$ë¥¼ ëœë¤í•˜ê²Œ ì„¤ì • (ì¼ë°˜ì ìœ¼ë¡œ 0ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥)
2.  **ê¸°ìš¸ê¸° ê³„ì‚°**: ë¹„ìš© í•¨ìˆ˜ $J(w, b)$ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°
3.  **ì—…ë°ì´íŠ¸**: $w$ì™€ $b$ë¥¼ ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™ (í•™ìŠµë¥  ì¡°ì ˆ)
4.  **ë°˜ë³µ**: ì›í•˜ëŠ” ìµœì†Œê°’ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ ìˆ˜í–‰

---

## ğŸ“Š í•™ìŠµë¥ ($\\alpha$) ì¡°ì • ë°©ë²•

í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ë©´ ğŸš€:

-   ë„ˆë¬´ í¬ê²Œ ì´ë™í•˜ì—¬ ìµœì†Œê°’ì„ ì§€ë‚˜ì³ë²„ë¦´ ìˆ˜ ìˆìŒ

í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ğŸŒ:

-   ìµœì ê°’ì— ë„ë‹¬í•˜ëŠ” ì†ë„ê°€ ëŠë ¤ì§

### ğŸ“Œ í•™ìŠµë¥ ì— ë”°ë¥¸ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ì°¨ì´

```
import numpy as np
import matplotlib.pyplot as plt

def gradient_descent(learning_rate, iterations):
    w = 5  # ì´ˆê¸°ê°’ ì„¤ì •
    w_history = []
    for _ in range(iterations):
        grad = 2 * w  # ì˜ˆì œ ë¹„ìš© í•¨ìˆ˜ J(w) = w^2ì˜ ê¸°ìš¸ê¸°
        w -= learning_rate * grad
        w_history.append(w)
    return w_history

iterations = 20
learning_rates = [0.01, 0.1, 0.5]

plt.figure(figsize=(8,6))
for lr in learning_rates:
    w_history = gradient_descent(lr, iterations)
    plt.plot(w_history, label=f'Learning Rate = {lr}')

plt.xlabel('Iterations')
plt.ylabel('w Value')
plt.title('Effect of Learning Rate on Gradient Descent')
plt.legend()
plt.show()
```

---

## âœ… ë§ˆë¬´ë¦¬

ì´ë²ˆ ê¸€ì—ì„œëŠ” **ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)** ì„ í™œìš©í•˜ì—¬ ë¹„ìš© í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ê¸€ì—ì„œëŠ” **ê²½ì‚¬ í•˜ê°•ë²•ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•**ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ğŸš€

âœ… **ìš”ì•½**

-   **ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)** ì€ ë¹„ìš© í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜
-   ë¹„ìš© í•¨ìˆ˜ $J(w, b)$ ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ **ê°€ì¥ ê°€íŒŒë¥¸ ë°©í–¥ìœ¼ë¡œ ì´ë™**
-   ì—…ë°ì´íŠ¸ ê³µì‹: $w := w - \\alpha \\frac{\\partial J}{\\partial w}$, $b := b - \\alpha \\frac{\\partial J}{\\partial b}$
-   í•™ìŠµë¥ ($\\alpha$)ì„ ì ì ˆíˆ ì¡°ì •í•˜ì—¬ ìµœì ì˜ í•™ìŠµ ì†ë„ë¥¼ ìœ ì§€í•´ì•¼ í•¨

ë‹¤ìŒ ê¸€ì—ì„œëŠ” **ì‹¤ì œ ê²½ì‚¬ í•˜ê°•ë²• êµ¬í˜„ ì½”ë“œ**ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤! ğŸ˜‰
