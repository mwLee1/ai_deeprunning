# 🧠 Logistic Regression: 경사 하강법(Gradient Descent) 이해하기

## 🚀 소개

이전 글에서 **로지스틱 회귀(Logistic Regression)** 의 비용 함수(Cost Function)를 정의하고, 손실 함수(Loss Function)와의 차이를 살펴보았습니다. 이번 글에서는 **경사 하강법(Gradient Descent)** 을 사용하여 모델을 학습하는 방법을 설명하겠습니다.

## 📌 경사 하강법이란?

경사 하강법(Gradient Descent)은 비용 함수 J(w, b)를 최소화하는 최적의 **가중치(w)와 바이어스(b)** 를 찾는 알고리즘입니다.<br/>
![Gradient Descent](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/gradient-descent-convex-function.png)

![Gradient Descent](https://mlpills.dev/wp-content/uploads/2022/10/CaIB7lz-h.jpg)

### 📊 비용 함수와 최적화 목표

비용 함수 $J(w, b)$ 는 **모든 훈련 샘플에서 손실 함수의 평균** 입니다.  
$(J(w, b) = - \\frac{1}{m} \\sum\_{i=1}^{m} \\big(y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)})\\big))$

이 비용 함수를 최소화하는 것이 **로지스틱 회귀 모델 학습의 핵심 목표** 입니다.

---

## 🎯 경사 하강법의 직관적 이해

비용 함수 $J(w, b)$ 는 파라미터터($w$, $b$)에 따라 변화하는 **곡면 형태의 함수**입니다.

-   비용 함수의 **최솟값**을 찾는 것이 목표입니다.
-   최솟값을 찾기 위해, **가장 가파른 내리막 방향으로 이동**해야 합니다.
-   이를 위해 비용 함수의 **미분(Gradient, 기울기)** 을 계산하여 **가장 빠르게 감소하는 방향**을 찾습니다.

### 📌 비용 함수의 그래프 예시

```
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def cost_function(w, b):
    return np.log(1 + np.exp(-(w * 3 + b)))  # 예제 비용 함수

w_values = np.linspace(-5, 5, 100)
b_values = np.linspace(-5, 5, 100)
W, B = np.meshgrid(w_values, b_values)
J = cost_function(W, B)

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(W, B, J, cmap='viridis')
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_zlabel('J(w, b)')
ax.set_title('Cost Function J(w, b)')
plt.show()
```

---

## 🔢 경사 하강법의 수식

경사 하강법은 아래의 수식을 반복하여 가중치($w$)와 바이어스($b$)를 업데이트합니다.

### 📌 경사 하강법 알고리즘

#### **가중치($w$) 업데이트**

$(w := w - \\alpha \\frac{\\partial J}{\\partial w})$

#### **바이어스($b$) 업데이트**

$(b := b - \\alpha \\frac{\\partial J}{\\partial b})$

여기서,

-   alpha (alpha): **학습률(Learning Rate)**, 한 번에 이동하는 크기
-   ∂J/∂w: $w$에 대한 **비용 함수의 기울기**
-   ∂J/∂b: $b$에 대한 **비용 함수의 기울기**

---

## 🔥 경사 하강법의 동작 방식

1.  **초기화**: $w$와 $b$를 랜덤하게 설정 (일반적으로 0으로 설정 가능)
2.  **기울기 계산**: 비용 함수 $J(w, b)$의 기울기를 계산
3.  **업데이트**: $w$와 $b$를 기울기의 반대 방향으로 이동 (학습률 조절)
4.  **반복**: 원하는 최소값에 도달할 때까지 반복 수행

---

## 📊 학습률($\\alpha$) 조정 방법

학습률이 너무 크면 🚀:

-   너무 크게 이동하여 최소값을 지나쳐버릴 수 있음

학습률이 너무 작으면 🐌:

-   최적값에 도달하는 속도가 느려짐

### 📌 학습률에 따른 경사 하강법의 차이

```
import numpy as np
import matplotlib.pyplot as plt

def gradient_descent(learning_rate, iterations):
    w = 5  # 초기값 설정
    w_history = []
    for _ in range(iterations):
        grad = 2 * w  # 예제 비용 함수 J(w) = w^2의 기울기
        w -= learning_rate * grad
        w_history.append(w)
    return w_history

iterations = 20
learning_rates = [0.01, 0.1, 0.5]

plt.figure(figsize=(8,6))
for lr in learning_rates:
    w_history = gradient_descent(lr, iterations)
    plt.plot(w_history, label=f'Learning Rate = {lr}')

plt.xlabel('Iterations')
plt.ylabel('w Value')
plt.title('Effect of Learning Rate on Gradient Descent')
plt.legend()
plt.show()
```

---

## ✅ 마무리

이번 글에서는 **경사 하강법(Gradient Descent)** 을 활용하여 비용 함수를 최소화하는 방법을 학습했습니다.

다음 글에서는 **경사 하강법을 코드로 구현하는 방법**을 살펴보겠습니다. 🚀

✅ **요약**

-   **경사 하강법(Gradient Descent)** 은 비용 함수를 최소화하는 최적화 알고리즘
-   비용 함수 $J(w, b)$ 의 기울기를 계산하여 **가장 가파른 방향으로 이동**
-   업데이트 공식: $w := w - \\alpha \\frac{\\partial J}{\\partial w}$, $b := b - \\alpha \\frac{\\partial J}{\\partial b}$
-   학습률($\\alpha$)을 적절히 조정하여 최적의 학습 속도를 유지해야 함

다음 글에서는 **실제 경사 하강법 구현 코드**를 살펴보겠습니다! 😉
