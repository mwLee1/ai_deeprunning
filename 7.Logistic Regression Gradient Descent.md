

# 🧠 Logistic Regression: 경사 하강법(Gradient Descent)의 미분적 구현

## 📌 서론: 왜 경사 하강법이 중요한가?

**로지스틱 회귀(Logistic Regression)** 는 확률적 분류 문제에서 가장 널리 사용되는 선형 모델입니다. 핵심은 다음과 같습니다:

* 모델이 예측한 확률값 $\hat{y}$ 와 실제 정답 $y$ 사이의 오차를 측정하고,
* 이 오차를 최소화하는 방향으로 모델의 **파라미터(\$w, b\$)** 를 반복적으로 갱신하는 것입니다.

이를 가능하게 해주는 것이 바로 **경사 하강법(Gradient Descent)** 입니다.
이번 글에서는 로지스틱 회귀의 **미분 그래프를 기반으로 한 경사 하강법을 수학적으로 분석**하고, **직접 구현**합니다.

---

## 📚 1. 로지스틱 회귀의 모델 구조

### ✅ 모델 정의

모델의 예측값은 시그모이드 함수를 통해 계산됩니다:

$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

여기서, \$ z \$ 는 선형 결합입니다:

$$
z = \mathbf{w}^\top \mathbf{x} + b = w_1 x_1 + w_2 x_2 + b
$$

### ✅ 손실 함수

모델이 출력한 확률값 \$\hat{y}\$ 와 실제 값 \$y \in {0,1}\$ 사이의 불일치를 다음의 **로지스틱 손실(Logistic Loss)** 함수로 측정합니다:

$$
L(\hat{y}, y) = -\left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]
$$

이 손실은 **크로스 엔트로피(Cross Entropy)** 로 해석되며, 확률 분포 간의 차이를 정량화합니다.

---

## 🔄 2. 계산 그래프(Computation Graph)를 활용한 미분 흐름

경사 하강법은 손실 함수 \$L\$ 을 **파라미터 공간에서 최소화**하는 최적화 알고리즘입니다. 이를 위해 **연쇄 법칙(Chain Rule)** 을 적용하여 파라미터별로 **기울기(Gradient)** 를 계산해야 합니다.

[mermaid live editor](https://mermaid.live)
```
graph LR
  %% === 정방향 (Forward Pass) 수평 방향 ===
  x1[x₁] --> z
  w1[w₁] --> z
  x2[x₂] --> z
  w2[w₂] --> z
  b[b]   --> z

  z["z = w₁·x₁ + w₂·x₂ + b"] --> a["a = σ(z)"]
  a --> L["L(a, y)"]
  y[y] --> L
```


### 📈 Forward Pass (정방향 계산)

정방향 계산은 다음과 같은 연산 흐름을 따릅니다:

1. \$z = w\_1 x\_1 + w\_2 x\_2 + b\$
2. \$a = \sigma(z) = \frac{1}{1 + e^{-z}}\$
3. \$L = -\left\[y \log a + (1 - y) \log(1 - a)\right]\$

### 🔁 Backward Pass (역전파 계산)

정방향 흐름을 따라 얻은 결과를 바탕으로, 다음과 같이 역전파를 수행합니다:

#### (1) 손실에 대한 시그모이드 출력 \$a\$ 의 도함수:

$$
\frac{\partial L}{\partial a} = - \left( \frac{y}{a} - \frac{1 - y}{1 - a} \right)
$$

#### (2) 시그모이드 출력 \$a\$ 에 대한 \$z\$ 의 도함수:

$$
\frac{\partial a}{\partial z} = a(1 - a)
$$

#### (3) 연쇄 법칙 적용 → \$\frac{\partial L}{\partial z}\$

$$
\frac{\partial L}{\partial z} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z}
$$

하지만 위 계산을 직접 하지 않아도, **수학적으로 단순화**가 가능합니다:

$$
\frac{\partial L}{\partial z} = a - y
$$

이는 로지스틱 회귀의 고유한 성질로, 매우 중요한 직관입니다. (즉, "예측값 - 정답")

#### (4) 각 파라미터에 대한 도함수

$$
\frac{\partial L}{\partial w_1} = (a - y) \cdot x_1,\quad
\frac{\partial L}{\partial w_2} = (a - y) \cdot x_2,\quad
\frac{\partial L}{\partial b} = (a - y)
$$

---

## 📉 3. 경사 하강법 파라미터 업데이트

이제 계산된 기울기를 바탕으로 각 파라미터를 다음과 같이 갱신합니다:

$$
w_i := w_i - \alpha \cdot \frac{\partial L}{\partial w_i},\quad
b := b - \alpha \cdot \frac{\partial L}{\partial b}
$$

* \$\alpha\$ 는 **학습률(Learning Rate)** 으로, 경사의 크기에 곱해져 한 번의 업데이트 폭을 조절합니다.
* 이 과정은 **매 반복(epoch)** 마다 전체 데이터(또는 배치)에 대해 반복됩니다.

---

## 💻 4. 단일 샘플 기반의 경사 하강법 구현

```python
import numpy as np

# 초기 파라미터
w1, w2, b = np.random.randn(), np.random.randn(), np.random.randn()
x1, x2 = 0.5, 1.2   # 입력 벡터 x
y = 1               # 정답 레이블
alpha = 0.01        # 학습률

# 정방향 패스
z = w1 * x1 + w2 * x2 + b
a = 1 / (1 + np.exp(-z))  # 시그모이드 함수

# 역방향 패스
dz = a - y
dw1 = dz * x1
dw2 = dz * x2
db = dz

# 파라미터 업데이트
w1 -= alpha * dw1
w2 -= alpha * dw2
b -= alpha * db

print(f"Updated Parameters -> w1: {w1:.4f}, w2: {w2:.4f}, b: {b:.4f}")
```

> 이 구현은 **단일 샘플(Stochastic Gradient Descent)** 에 기반하며, 실제 학습에서는 일반적으로 다중 샘플(미니 배치 또는 전체 데이터)을 사용합니다.

---

## 🧾 요약 정리

| 항목        | 내용                                                                 |
| --------- | ------------------------------------------------------------------ |
| 모델        | \$\hat{y} = \sigma(\mathbf{w}^\top \mathbf{x} + b)\$             |
| 손실 함수     | 크로스 엔트로피: \$-\[y\log a + (1 - y)\log(1 - a)]\$                   |
| 역전파 핵심    | \$\frac{\partial L}{\partial z} = a - y\$                        |
| 파라미터 업데이트 | \$w\_i := w\_i - \alpha \cdot \frac{\partial L}{\partial w\_i}\$ |

---

## 🔭 다음 단계 예고

다음 글에서는 **다중 샘플(batch)** 기반의 경사 하강법, 즉 **미니 배치 확률적 경사 하강법(Mini-batch SGD)** 을 통해 로지스틱 회귀를 확장하는 방법을 다룰 예정입니다.
또한 **벡터화된 연산**, **행렬 형태의 데이터 처리**, **NumPy 최적화 구현** 도 함께 설명합니다.




















