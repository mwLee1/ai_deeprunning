

# 🧠 Logistic Regression의 비용 함수(Cost Function)

## 1. 🔍 서론

로지스틱 회귀(Logistic Regression)는 이진 분류(Binary Classification)에 널리 사용되는 통계 기반 기법입니다. 이전 글에서는 **시그모이드 함수(Sigmoid Function)** 를 통해 로지스틱 회귀가 확률적 출력을 생성하는 방식을 살펴보았습니다. 본 글에서는 이 확률 예측을 기반으로 **모델 학습의 핵심 목표인 비용 함수(Cost Function)** 를 어떻게 정의하고 최적화하는지를 수학적이고 직관적으로 분석합니다.

---

## 2. 🎯 비용 함수란 무엇인가?

비용 함수(Cost Function)란 주어진 파라미터 \$w, b\$에 대해 모델이 예측한 결과와 실제 정답 간의 **오차를 정량화하는 함수**입니다. 머신러닝 모델 학습의 목적은 이 비용 함수를 최소화하는 파라미터를 찾는 것입니다.

로지스틱 회귀에서 예측값 \$\hat{y}\$는 다음과 같이 정의됩니다:

$\hat{y} = \sigma(z) = \sigma(w^T x + b), \quad \text{where } \sigma(z) = \frac{1}{1 + e^{-z}}$

---

## 3. 📉 왜 MSE는 로지스틱 회귀에 부적합한가?

선형 회귀에서는 예측값과 실제값 간의 오차 제곱을 사용하는 **평균 제곱 오차(MSE)** 를 손실 함수로 사용합니다:

$L_{\text{MSE}}(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2$

그러나 로지스틱 회귀에서 MSE를 사용하면 다음과 같은 문제점이 발생합니다:

* **비용 함수가 비볼록(Non-Convex)** 하여 경사 하강법으로 최적화를 수행할 때 **지역 최소값(Local Minima)** 에 빠질 위험이 있음
* 시그모이드 함수의 **비선형성** 과 MSE의 조합은 미분 결과가 불안정하고, **학습 속도가 느리거나 수렴하지 않을 수 있음**

> ✅ 따라서, **볼록 함수(convex function)** 를 사용해야 **전역 최소값(global minimum)** 으로 수렴이 보장됩니다.

---

## 4. 🧮 Binary Cross-Entropy (이진 교차 엔트로피)

로지스틱 회귀에서는 아래와 같은 **로그 손실 함수(Log Loss)** 또는 **이진 교차 엔트로피(Binary Cross-Entropy)** 를 사용합니다:

$L(\hat{y}, y) = - \left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]$

이 손실 함수는 다음의 두 상황을 각각 다르게 반영합니다:

* **\$y=1\$일 때**:

  $L(\hat{y}, 1) = -\log(\hat{y}) \quad \Rightarrow \quad \hat{y} \to 1 \text{ 일수록 Loss ↓}$

* **\$y=0\$일 때**:

  $L(\hat{y}, 0) = -\log(1 - \hat{y}) \quad \Rightarrow \quad \hat{y} \to 0 \text{ 일수록 Loss ↓}$

이는 확률 모델의 관점에서 **Maximum Likelihood Estimation (MLE)** 을 기반으로 한 로그 가능도 함수의 음수 형태로, 통계적으로도 정당화됩니다.

---

## 5. 📊 시각적 이해: 손실 함수 그래프

```python
import numpy as np
import matplotlib.pyplot as plt

def binary_cross_entropy(y, y_pred):
    return - (y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))

y_pred = np.linspace(0.01, 0.99, 100)
y1_loss = binary_cross_entropy(1, y_pred)
y0_loss = binary_cross_entropy(0, y_pred)

plt.plot(y_pred, y1_loss, label='Loss when y=1', color='red')
plt.plot(y_pred, y0_loss, label='Loss when y=0', color='blue')
plt.xlabel("Predicted Probability $\hat{y}$")
plt.ylabel("Loss $L(\hat{y}, y)$")
plt.title("Binary Cross Entropy Loss Function")
plt.legend()
plt.grid(True)
plt.show()
```

🔎 **그래프 해석**:

* \$y=1\$일 때, 예측값이 1에 가까울수록 손실이 감소
* \$y=0\$일 때, 예측값이 0에 가까울수록 손실이 감소
* \$\hat{y} = 0.5\$ 근처에서는 손실이 비교적 큰 값을 가짐 (모델의 불확실성이 큰 경우)

---

## 6. 📘 비용 함수 (Cost Function) 전체 정의

로지스틱 회귀의 **비용 함수 \$J(w, b)\$** 는 모든 학습 샘플에 대한 손실 함수의 평균입니다.

$J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) = - \frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)}) \right]$

여기서:

* \$m\$은 총 샘플 수
* \$\hat{y}^{(i)} = \sigma(w^T x^{(i)} + b)\$
* \$y^{(i)} \in {0, 1}\$

즉, 비용 함수는 모델의 예측이 전체적으로 실제 값과 얼마나 가까운지를 측정합니다.

---

## 7. 🧠 이론적 정당성: MLE 관점

로그 손실 함수는 **Maximum Likelihood Estimation** 을 기반으로 도출됩니다.

* 베르누이 분포로부터 데이터를 생성한다고 가정하면:

$P(y | x; w, b) = \hat{y}^y (1 - \hat{y})^{1 - y}$

* 전체 likelihood는:

$\mathcal{L}(w, b) = \prod_{i=1}^{m} \hat{y}^{(i)y^{(i)}} (1 - \hat{y}^{(i)})^{1 - y^{(i)}}$

* 로그를 취하면 (log-likelihood):

$\log \mathcal{L}(w, b) = \sum_{i=1}^{m} \left[ y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)}) \right]$

* 이를 **최대화**하는 것은 로그 손실을 **최소화**하는 것과 동일합니다.

---

## 8. 🛠️ 모델 학습: 비용 함수 최적화

로지스틱 회귀 모델 학습의 궁극적인 목표는 다음을 만족하는 파라미터 \$w^*, b^*\$ 를 찾는 것입니다:

$\(w^*, b^*) = \arg\min_{w, b} J(w, b)$

이때 사용되는 최적화 기법은 대표적으로 **경사 하강법(Gradient Descent)** 입니다. 이는 다음 글에서 심도 있게 다룰 예정입니다.

---

## ✅ 요약 정리

| 항목                          | 설명                                     |
| --------------------------- | -------------------------------------- |
| **손실 함수 \$L(\hat{y}, y)\$** | 개별 샘플의 예측 오차                           |
| **비용 함수 \$J(w, b)\$**       | 전체 샘플의 평균 손실                           |
| **사용 손실 함수**                | Binary Cross-Entropy (이진 교차 엔트로피)      |
| **MSE 미사용 이유**              | 비볼록성 → 학습 수렴 어려움                       |
| **최적화 대상**                  | 비용 함수 \$J(w, b)\$ 를 최소화하는 \$w\$, \$b\$ |
| **정당성**                     | 확률론적 해석 (MLE 기반)으로 유도됨                 |

---

## 📌 다음 글 예고

다음 글에서는 **비용 함수의 기울기(Gradient)를 계산하고**, 이를 통해 **경사 하강법을 적용한 파라미터 업데이트 방식**을 다루겠습니다. 실제 학습이 어떻게 이루어지는지 수식과 함께 상세히 설명드릴 예정입니다.

---

