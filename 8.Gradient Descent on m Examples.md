

# ğŸ§  Logistic Regression: ë‹¤ì¤‘ ìƒ˜í”Œ ê¸°ë°˜ ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent on Multiple Examples)

## ğŸš€ ê°œìš”

ì´ì „ ê¸€ì—ì„œëŠ” ë‹¨ì¼ ìƒ˜í”Œ(Stochastic Case)ì— ëŒ€í•œ **ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)** ì˜ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ì—ì„œëŠ” **ì—¬ëŸ¬ ìƒ˜í”Œì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ë²¡í„°í™”ëœ(ë¯¸ë‹ˆë°°ì¹˜ ë˜ëŠ” ì „ëŸ‰) ê²½ì‚¬ í•˜ê°•ë²•**ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.

ì´ë²ˆ ê¸€ì—ì„œëŠ” \$m\$ê°œì˜ ìƒ˜í”Œì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” **ë²¡í„°í™”ëœ ê²½ì‚¬ í•˜ê°•ë²•(Vectorized Gradient Descent)** ì„ ìˆ˜ì‹ì , êµ¬í˜„ì ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ“Œ 1. ë¹„ìš© í•¨ìˆ˜: Cross-Entropy Loss over m Samples

\$m\$ê°œì˜ í›ˆë ¨ ìƒ˜í”Œ \$(\mathbf{x}^{(i)}, y^{(i)})\$ì— ëŒ€í•´, ì „ì²´ ë¹„ìš© í•¨ìˆ˜ \$J(w, b)\$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
$$

ì—¬ê¸°ì„œ:

* \$\hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(\mathbf{w}^\top \mathbf{x}^{(i)} + b)\$
* \$m\$: ìƒ˜í”Œ ìˆ˜
* \$\sigma(z) = \dfrac{1}{1 + e^{-z}}\$: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜

---

## ğŸ¯ 2. Gradient Derivation (ë²¡í„°í™”ëœ ë„í•¨ìˆ˜ ê³„ì‚°)

ê²½ì‚¬ í•˜ê°•ë²•ì„ ìœ„í•´ ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë¥¼ **ë²¡í„°í™”(vectorization)** í•˜ì—¬ ìˆ˜ì‹í™”í•˜ë©´:

### ğŸ”¹ ì˜ˆì¸¡ê°’ í–‰ë ¬

$$
\mathbf{Z} = X \mathbf{w} + b,\quad \mathbf{A} = \sigma(\mathbf{Z})
$$

* \$X \in \mathbb{R}^{m \times n}\$: ì…ë ¥ ìƒ˜í”Œ í–‰ë ¬
* \$\mathbf{w} \in \mathbb{R}^{n \times 1}\$: ê°€ì¤‘ì¹˜ ë²¡í„°
* \$\mathbf{A} \in \mathbb{R}^{m \times 1}\$: ì˜ˆì¸¡ í™•ë¥  ë²¡í„°

### ğŸ”¹ ê·¸ë˜ë””ì–¸íŠ¸ ê³µì‹ (ë„í•¨ìˆ˜)

$$
\begin{align*}
\frac{\partial J}{\partial \mathbf{w}} &= \frac{1}{m} X^\top (\mathbf{A} - \mathbf{Y}) \\
\frac{\partial J}{\partial b} &= \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})
\end{align*}
$$

ì—¬ê¸°ì„œ \$\mathbf{Y} \in \mathbb{R}^{m \times 1}\$ ì€ ì‹¤ì œ ì •ë‹µ ë²¡í„°ì…ë‹ˆë‹¤.

> âš ï¸ ì´ ì‹ì€ ë§¤ìš° ì¤‘ìš”í•˜ë©°, ëª¨ë“  ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬(PyTorch, TensorFlow ë“±)ì—ì„œ ë™ì¼í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤.

---

## ğŸ” 3. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (Gradient Descent Rule)

ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•œ íŒŒë¼ë¯¸í„° ê°±ì‹ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
\begin{align*}
\mathbf{w} &:= \mathbf{w} - \alpha \cdot \frac{\partial J}{\partial \mathbf{w}} \\
b &:= b - \alpha \cdot \frac{\partial J}{\partial b}
\end{align*}
$$

* \$\alpha\$: í•™ìŠµë¥  (learning rate)

---

## ğŸ’» 4. êµ¬í˜„: ë‹¤ì¤‘ ìƒ˜í”Œ ê¸°ë°˜ ë¡œì§€ìŠ¤í‹± íšŒê·€

```python
import numpy as np

# ë°ì´í„° ì¤€ë¹„ (m=4, n=2)
X = np.array([[0.5, 1.2],
              [1.1, 2.4],
              [0.8, 1.5],
              [1.5, 3.0]])   # shape: (4, 2)
Y = np.array([[1], [0], [1], [0]])  # shape: (4, 1)

# ì´ˆê¸°í™”
np.random.seed(42)
w = np.random.randn(2, 1)  # shape: (2, 1)
b = np.random.randn()
alpha = 0.01
m = X.shape[0]

# Forward Pass
Z = np.dot(X, w) + b             # (m x 1)
A = 1 / (1 + np.exp(-Z))         # sigmoid(Z)

# Backward Pass
dZ = A - Y                       # (m x 1)
dW = (1/m) * np.dot(X.T, dZ)     # (n x 1)
dB = (1/m) * np.sum(dZ)          # scalar

# Update Parameters
w -= alpha * dW
b -= alpha * dB

print("Updated weights:\n", w)
print("Updated bias:\n", b)
```

---

## ğŸ“Š 5. ë¹„ìš© í•¨ìˆ˜ ì‹œê°í™” (Gradient Descent Progress)

```python
import matplotlib.pyplot as plt

def cost_function_plot():
    iterations = np.arange(1, 51)
    cost_values = np.exp(-0.1 * iterations) + 0.05 * np.random.randn(50)  # ê°€ìƒì˜ ë¹„ìš© ê°ì†Œ
    plt.plot(iterations, cost_values, marker='o')
    plt.xlabel("Iterations")
    plt.ylabel("Cost J(w, b)")
    plt.title("Convergence of Gradient Descent")
    plt.grid(True)
    plt.show()

cost_function_plot()
```

---

## âœ… ìš”ì•½ ì •ë¦¬

| í•­ëª©      | ì„¤ëª…                                                                                                                                                               |
| ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ëª¨ë¸ êµ¬ì¡°   | \$ \hat{y} = \sigma(\mathbf{w}^\top \mathbf{x} + b) \$                                                                                                           |
| ë¹„ìš© í•¨ìˆ˜   | \$ J(w, b) = -\dfrac{1}{m} \sum\_{i=1}^m \left\[ y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)}) \right] \$                                  |
| ë„í•¨ìˆ˜     | \$ \dfrac{\partial J}{\partial \mathbf{w}} = \dfrac{1}{m} X^\top (\mathbf{A} - \mathbf{Y}) \$,  \$ \dfrac{\partial J}{\partial b} = \dfrac{1}{m} \sum (A - Y) \$ |
| ì—…ë°ì´íŠ¸ ê·œì¹™ | \$ \mathbf{w} := \mathbf{w} - \alpha \dfrac{\partial J}{\partial \mathbf{w}},\quad b := b - \alpha \dfrac{\partial J}{\partial b} \$                             |

---

## ğŸ”­ ë‹¤ìŒ ì£¼ì œ ì˜ˆê³ 

ë‹¤ìŒ ê¸€ì—ì„œëŠ”:

* ë°˜ë³µ ë£¨í”„(epoch)
* **ë²¡í„°í™”ëœ ë¹„ìš© í•¨ìˆ˜ í‰ê°€**
* **ë¯¸ë‹ˆë°°ì¹˜ ê¸°ë°˜ SGD**
* **ì •ê·œí™” í•­ (Regularization)** í¬í•¨

ë“±ì„ í†µí•´ ë”ìš± ì¼ë°˜ì ì¸ **ë¡œì§€ìŠ¤í‹± íšŒê·€ ìµœì í™” í”„ë ˆì„ì›Œí¬**ë¡œ í™•ì¥í•´ ë‚˜ê°€ê² ìŠµë‹ˆë‹¤.

