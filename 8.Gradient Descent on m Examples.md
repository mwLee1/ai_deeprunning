

# 🧠 Logistic Regression: 다중 샘플 기반 경사 하강법 (Gradient Descent on Multiple Examples)

## 🚀 개요

이전 글에서는 단일 샘플(Stochastic Case)에 대한 **로지스틱 회귀(Logistic Regression)** 의 경사 하강법을 살펴보았습니다. 하지만 실제 머신러닝 모델 훈련에서는 **여러 샘플을 동시에 처리하는 벡터화된(미니배치 또는 전량) 경사 하강법**이 필수적입니다.

이번 글에서는 \$m\$개의 샘플을 한 번에 처리하는 **벡터화된 경사 하강법(Vectorized Gradient Descent)** 을 수식적, 구현적으로 상세히 설명합니다.

---

## 📌 1. 비용 함수: Cross-Entropy Loss over m Samples

\$m\$개의 훈련 샘플 \$(\mathbf{x}^{(i)}, y^{(i)})\$에 대해, 전체 비용 함수 \$J(w, b)\$는 다음과 같이 정의됩니다:

$$
J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
$$

여기서:

* \$\hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(\mathbf{w}^\top \mathbf{x}^{(i)} + b)\$
* \$m\$: 샘플 수
* \$\sigma(z) = \dfrac{1}{1 + e^{-z}}\$: 시그모이드 함수

---

## 🎯 2. Gradient Derivation (벡터화된 도함수 계산)

경사 하강법을 위해 각 파라미터에 대한 기울기를 계산합니다. 이를 **벡터화(vectorization)** 하여 수식화하면:

### 🔹 예측값 행렬

$$
\mathbf{Z} = X \mathbf{w} + b,\quad \mathbf{A} = \sigma(\mathbf{Z})
$$

* \$X \in \mathbb{R}^{m \times n}\$: 입력 샘플 행렬
* \$\mathbf{w} \in \mathbb{R}^{n \times 1}\$: 가중치 벡터
* \$\mathbf{A} \in \mathbb{R}^{m \times 1}\$: 예측 확률 벡터

### 🔹 그래디언트 공식 (도함수)

$$
\begin{align*}
\frac{\partial J}{\partial \mathbf{w}} &= \frac{1}{m} X^\top (\mathbf{A} - \mathbf{Y}) \\
\frac{\partial J}{\partial b} &= \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})
\end{align*}
$$

여기서 \$\mathbf{Y} \in \mathbb{R}^{m \times 1}\$ 은 실제 정답 벡터입니다.

> ⚠️ 이 식은 매우 중요하며, 모든 딥러닝 프레임워크(PyTorch, TensorFlow 등)에서 동일하게 사용됩니다.

---

## 🔁 3. 파라미터 업데이트 (Gradient Descent Rule)

경사 하강법을 통한 파라미터 갱신은 다음과 같습니다:

$$
\begin{align*}
\mathbf{w} &:= \mathbf{w} - \alpha \cdot \frac{\partial J}{\partial \mathbf{w}} \\
b &:= b - \alpha \cdot \frac{\partial J}{\partial b}
\end{align*}
$$

* \$\alpha\$: 학습률 (learning rate)

---

## 💻 4. 구현: 다중 샘플 기반 로지스틱 회귀

```python
import numpy as np

# 데이터 준비 (m=4, n=2)
X = np.array([[0.5, 1.2],
              [1.1, 2.4],
              [0.8, 1.5],
              [1.5, 3.0]])   # shape: (4, 2)
Y = np.array([[1], [0], [1], [0]])  # shape: (4, 1)

# 초기화
np.random.seed(42)
w = np.random.randn(2, 1)  # shape: (2, 1)
b = np.random.randn()
alpha = 0.01
m = X.shape[0]

# Forward Pass
Z = np.dot(X, w) + b             # (m x 1)
A = 1 / (1 + np.exp(-Z))         # sigmoid(Z)

# Backward Pass
dZ = A - Y                       # (m x 1)
dW = (1/m) * np.dot(X.T, dZ)     # (n x 1)
dB = (1/m) * np.sum(dZ)          # scalar

# Update Parameters
w -= alpha * dW
b -= alpha * dB

print("Updated weights:\n", w)
print("Updated bias:\n", b)
```

---

## 📊 5. 비용 함수 시각화 (Gradient Descent Progress)

```python
import matplotlib.pyplot as plt

def cost_function_plot():
    iterations = np.arange(1, 51)
    cost_values = np.exp(-0.1 * iterations) + 0.05 * np.random.randn(50)  # 가상의 비용 감소
    plt.plot(iterations, cost_values, marker='o')
    plt.xlabel("Iterations")
    plt.ylabel("Cost J(w, b)")
    plt.title("Convergence of Gradient Descent")
    plt.grid(True)
    plt.show()

cost_function_plot()
```

---

## ✅ 요약 정리

| 항목      | 설명                                                                                                                                                               |
| ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 모델 구조   | \$ \hat{y} = \sigma(\mathbf{w}^\top \mathbf{x} + b) \$                                                                                                           |
| 비용 함수   | \$ J(w, b) = -\dfrac{1}{m} \sum\_{i=1}^m \left\[ y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)}) \right] \$                                  |
| 도함수     | \$ \dfrac{\partial J}{\partial \mathbf{w}} = \dfrac{1}{m} X^\top (\mathbf{A} - \mathbf{Y}) \$,  \$ \dfrac{\partial J}{\partial b} = \dfrac{1}{m} \sum (A - Y) \$ |
| 업데이트 규칙 | \$ \mathbf{w} := \mathbf{w} - \alpha \dfrac{\partial J}{\partial \mathbf{w}},\quad b := b - \alpha \dfrac{\partial J}{\partial b} \$                             |

---

## 🔭 다음 주제 예고

다음 글에서는:

* 반복 루프(epoch)
* **벡터화된 비용 함수 평가**
* **미니배치 기반 SGD**
* **정규화 항 (Regularization)** 포함

등을 통해 더욱 일반적인 **로지스틱 회귀 최적화 프레임워크**로 확장해 나가겠습니다.

