

# 🧠 Derivatives with Computation Graphs: 계산 그래프 기반 미분 계산

## 1. 🚀 계산 그래프와 역전파는 무엇인가?

**계산 그래프(Computation Graph)** 는 수학적 계산을 **노드(Node)** 와 **연산(Edge)** 로 구성한 유향 비순환 그래프(DAG)입니다.
이는 복잡한 함수 연산을 분해해 **연쇄적으로 처리하고**, 특히 **미분을 효율적으로 계산**하는 데 활용됩니다.

딥러닝에서는 이를 기반으로 **정방향 전파(forward pass)** 와 **역방향 전파(backward pass)** 를 수행하여 모델 학습을 가능하게 합니다.

---

## 2. 📌 대상 함수: \$J = 3(a + bc)\$

이 함수는 아래와 같은 형태로 나뉩니다:

### 🔧 연산 분해

$$
\begin{aligned}
u &= bc \\
v &= a + u \\
J &= 3v
\end{aligned}
$$

→ 즉, **곱 → 덧셈 → 곱** 순서로 구성된 함수입니다.

---

## 3. 🔁 정방향 패스 (Forward Pass)

예를 들어:

* \$a = 5\$, \$b = 3\$, \$c = 2\$

이라면:

$$
\begin{aligned}
u &= bc = 3 \times 2 = 6 \\
v &= a + u = 5 + 6 = 11 \\
J &= 3v = 3 \times 11 = \boxed{33}
\end{aligned}
$$

---

## 4. 🔄 역방향 패스 (Backward Pass): 그래디언트 전파

우리는 이제 **출력 \$J\$에 대한 입력 변수들의 도함수 \$\frac{\partial J}{\partial a}, \frac{\partial J}{\partial b}, \frac{\partial J}{\partial c}\$** 를 계산합니다.
이 과정은 **체인 룰(chain rule)** 을 기반으로 한 **그래디언트 전파**입니다.

---

### 📘 개념적 도식
[Mermaid Live Editor](https://mermaid.live)
```
graph LR
    a[a = 5<br/>∂J/∂a = 3] --> v
    b[b = 3<br/>∂J/∂b = 6] --> u
    c[c = 2<br/>∂J/∂c = 9] --> u
    u[u = bc = 6<br/>∂J/∂u = 3] --> v
    v[v = a + u = 11<br/>∂J/∂v = 3] --> J[J = 3v = 33<br/>∂J/∂J = 1]

```

---

### ▶ 단계별 미분 계산

---

### 🔹 Step 1: \$\frac{\partial J}{\partial v}\$

$$
J = 3v \Rightarrow \frac{\partial J}{\partial v} = 3
$$

> 💡 "v가 1 증가하면 J는 3만큼 증가한다"

---

### 🔹 Step 2: \$\frac{\partial J}{\partial a}\$

$$
v = a + u \Rightarrow \frac{\partial v}{\partial a} = 1
\Rightarrow \frac{\partial J}{\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a} = 3 \cdot 1 = \boxed{3}
$$

---

### 🔹 Step 3: \$\frac{\partial J}{\partial u}\$

$$
\frac{\partial v}{\partial u} = 1
\Rightarrow \frac{\partial J}{\partial u} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} = 3 \cdot 1 = \boxed{3}
$$

---

### 🔹 Step 4: \$\frac{\partial J}{\partial b}\$

$$
u = bc \Rightarrow \frac{\partial u}{\partial b} = c = 2
\Rightarrow \frac{\partial J}{\partial b} = \frac{\partial J}{\partial u} \cdot \frac{\partial u}{\partial b} = 3 \cdot 2 = \boxed{6}
$$

---

### 🔹 Step 5: \$\frac{\partial J}{\partial c}\$

$$
\frac{\partial u}{\partial c} = b = 3
\Rightarrow \frac{\partial J}{\partial c} = \frac{\partial J}{\partial u} \cdot \frac{\partial u}{\partial c} = 3 \cdot 3 = \boxed{9}
$$

---

## 5. 🧠 Chain Rule의 구조적 적용

모든 도함수는 다음과 같은 연쇄 규칙을 따릅니다:

$$
\frac{\partial J}{\partial x} = \sum_{\text{경로}} \left( \prod_{\text{경로상의 노드}} \text{local gradient} \right)
$$

→ **모든 경로를 따라가며 곱하고, 경로가 여러 개인 경우 더함**
→ 이 구조는 합성함수, 병렬 노드, 브랜치 구조까지 일반화됩니다.

---

## 6. 📊 계산 그래프 시각화 (Forward + Gradient 포함)

```python
import networkx as nx
import matplotlib.pyplot as plt

a, b, c = 5, 3, 2
u = b * c
v = a + u
J = 3 * v

dJ_dv = 3
dJ_da = dJ_dv * 1
dJ_du = dJ_dv * 1
dJ_db = dJ_du * c
dJ_dc = dJ_du * b

G = nx.DiGraph()
G.add_edges_from([
    ("b", "u"), ("c", "u"),
    ("u", "v"), ("a", "v"),
    ("v", "J")
])
pos = {"a": (0, 2), "b": (0, 1), "c": (0, 0), "u": (1, 0.5), "v": (2, 1), "J": (3, 1)}
labels = {
    "a": f"a=5\n∂J/∂a={dJ_da}",
    "b": f"b=3\n∂J/∂b={dJ_db}",
    "c": f"c=2\n∂J/∂c={dJ_dc}",
    "u": f"u=bc=6",
    "v": f"v=a+u=11\n∂J/∂v=3",
    "J": f"J=3v=33"
}

plt.figure(figsize=(10, 4))
nx.draw(G, pos, with_labels=False, node_color='lightblue', node_size=3000, edge_color='gray')
nx.draw_networkx_labels(G, pos, labels, font_size=10)
plt.title("Computation Graph with Gradients", fontsize=14)
plt.axis('off')
plt.tight_layout()
plt.show()
```

---

## 7. ✅ 요약 정리

| 항목           | 설명                              |
| ------------ | ------------------------------- |
| **정방향 패스**   | 계산 그래프를 따라 입력에서 출력까지 값을 계산      |
| **역방향 패스**   | 출력에서 입력으로 체인 룰을 통해 그래디언트를 전파    |
| **연쇄 법칙**    | 복합 함수의 도함수를 단계적으로 곱해서 전파        |
| **노드별 도함수**  | 각 연산의 지역 도함수를 정의하면 전체 도함수 계산 가능 |
| **딥러닝에서 역할** | 손실 함수의 그래디언트를 파라미터로 역전파하여 학습 진행 |

