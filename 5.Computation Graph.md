

# 🧠 Computation Graph: 계산 그래프의 이론적 이해와 활용

## 1. 🚀 서론: 왜 계산 그래프가 중요한가?

신경망(Neural Networks)에서 학습은 크게 두 단계로 나뉩니다:

* **정방향 패스(Forward Pass)**: 입력 데이터를 받아 **출력을 계산**하는 단계
* **역방향 패스(Backward Pass)**: **출력에 대한 손실 함수의 도함수(Gradient)** 를 계산하여 파라미터를 업데이트하는 단계

이 두 과정을 체계적으로 계산하고 미분하기 위해 **계산 그래프(Computation Graph)** 를 사용합니다.

> 계산 그래프는 딥러닝 프레임워크(PyTorch, TensorFlow 등)의 핵심 개념인 **오토그래드(autograd)** 의 수학적 기반입니다.

---

## 2. 📌 계산 그래프란?

**계산 그래프(Computation Graph)** 는 복잡한 수학적 식을 **노드와 엣지로 구성된 유향 그래프(DAG)** 로 표현한 것입니다.

* **노드(Node)**: 변수나 중간 계산 값
* **엣지(Edge)**: 연산 (곱셈, 덧셈, 함수 등)의 흐름

이를 통해 **연산의 순서를 명확히 하며**,
**역전파 시 chain rule(연쇄 법칙)** 을 체계적으로 적용할 수 있습니다.

---

## 3. 🧮 예제 함수: \$J = 3(a + bc)\$

### ▶ 수식 분해 (Atomic Operations)

복잡한 함수 \$J = 3(a + bc)\$ 를 최소 단위 연산으로 분해:

$$
\begin{aligned}
u &= bc \\
v &= a + u \\
J &= 3v
\end{aligned}
$$

### ▶ 계산 그래프 구조 (Graph Representation)

```python
import networkx as nx
import matplotlib.pyplot as plt

# 1. 입력값 지정
a_val = 5
b_val = 3
c_val = 2

# 2. 계산 수행 (Forward Pass)
u_val = b_val * c_val        # u = b * c = 3 * 2 = 6
v_val = a_val + u_val        # v = a + u = 5 + 6 = 11
J_val = 3 * v_val            # J = 3 * v = 3 * 11 = 33

# 3. 계산 그래프 정의
G = nx.DiGraph()
G.add_edges_from([
    ("b", "u"), ("c", "u"),     # u = bc
    ("a", "v"), ("u", "v"),     # v = a + u
    ("v", "J")                  # J = 3v
])

# 4. 노드 위치
pos = {
    "a": (0, 2),
    "b": (0, 1),
    "c": (0, 0),
    "u": (1, 0.5),
    "v": (2, 1),
    "J": (3, 1)
}

# 5. 수식과 계산 결과 포함한 라벨
labels = {
    "a": f"a = {a_val}",
    "b": f"b = {b_val}",
    "c": f"c = {c_val}",
    "u": f"u = b×c = {b_val}×{c_val} = {u_val}",
    "v": f"v = a+u = {a_val}+{u_val} = {v_val}",
    "J": f"J = 3×v = 3×{v_val} = {J_val}"
}

# 6. 그래프 시각화
plt.figure(figsize=(10, 4))
nx.draw(
    G, pos, with_labels=False,
    node_color='skyblue', edge_color='gray',
    node_size=3000, font_size=10
)

# 수식 라벨 출력
nx.draw_networkx_labels(G, pos, labels, font_size=10)

plt.title("Computation Graph with Calculated Values", fontsize=14)
plt.axis('off')
plt.tight_layout()
plt.show()

```

---

## 4. 📊 정방향 패스 (Forward Pass)

### ▶ 입력 예시

$$
a = 5, \quad b = 3, \quad c = 2
$$

### ▶ 단계별 계산

$$
\begin{aligned}
u &= bc = 3 \times 2 = 6 \\
v &= a + u = 5 + 6 = 11 \\
J &= 3v = 3 \times 11 = \boxed{33}
\end{aligned}
$$

### ▶ 정방향 패스의 의미

* 계산 흐름이 **입력 → 출력 방향**으로 진행됨
* 각 중간 노드의 **값을 캐시(Cache)** 해두면 **역방향 패스에서 재사용 가능**

---

## 5. 🔁 역방향 패스를 위한 준비

신경망 학습은 손실 함수 \$J\$ 를 최소화하기 위해 파라미터(예: \$w\$)에 대한 **도함수 \$\frac{\partial J}{\partial w}\$** 를 계산해야 합니다.

계산 그래프를 사용하면 **연쇄법칙(chain rule)** 을 다음처럼 체계적으로 적용할 수 있습니다:

$$
\frac{\partial J}{\partial a} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial a}, \quad
\frac{\partial J}{\partial b} = \frac{\partial J}{\partial v} \cdot \frac{\partial v}{\partial u} \cdot \frac{\partial u}{\partial b}, \quad \text{etc.}
$$

→ 이 과정은 **다음 글 (Backward Pass)** 에서 자세히 다룹니다.

---

## 6. 📘 계산 그래프의 장점

| 장점          | 설명                               |
| ----------- | -------------------------------- |
| **미분 자동화**  | 연산 순서를 구조화하여 chain rule 적용을 자동화  |
| **메모리 효율성** | 중간 연산 값을 캐시 → 역전파 시 재활용          |
| **병렬화 가능성** | 그래프 구조 기반으로 연산 최적화 가능            |
| **모듈화**     | 각 연산 단위로 미분 공식만 정의하면 전체 미분 계산 가능 |

> PyTorch나 TensorFlow의 `autograd` 시스템은 내부적으로 모든 연산을 계산 그래프로 추적하고 역전파 시 그래디언트를 자동 계산합니다.

---

## ✅ 요약

| 개념         | 설명                                   |
| ---------- | ------------------------------------ |
| **계산 그래프** | 수식을 노드 기반 유향 그래프로 표현하여 미분을 체계화       |
| **정방향 패스** | 입력을 기반으로 중간 값과 최종 출력 \$J\$ 계산        |
| **역방향 패스** | \$J\$에 대한 각 노드의 미분값을 chain rule로 역추적 |
| **활용**     | 신경망 학습, 자동 미분, 오토그래드, 역전파 등          |

---

## 🔍 다음 글 예고: Backward Pass (역방향 전파)

다음 글에서는 위에서 만든 계산 그래프를 기반으로 **각 변수에 대한 도함수**
즉, \$\frac{\partial J}{\partial a}\$, \$\frac{\partial J}{\partial b}\$, \$\frac{\partial J}{\partial c}\$ 등을
**수작업으로 역전파(Backpropagation) 방식으로 계산**해보겠습니다.

이 과정을 이해하면, **딥러닝 학습의 수학적 기반**을 완벽히 이해하게 됩니다.


