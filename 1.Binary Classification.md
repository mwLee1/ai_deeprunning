# 🧠 Binary Classification를 위한 로지스틱 회귀: 이론적 기초

## 🚀 서론

**로지스틱 회귀(Logistic Regression)** 는 이진 분류(Binary Classification) 문제를 해결하기 위한 가장 기초적이고 핵심적인 알고리즘입니다. 이름에 "회귀"가 들어가 있지만, 실제로는 **분류(Classification)** 모델입니다.

이 글에서는 로지스틱 회귀를 **신경망 구현의 관점에서** 바라보며, 정방향 전파(Forward Propagation), 역방향 전파(Backward Propagation) 같은 딥러닝의 핵심 요소들이 어떻게 작동하는지를 수학적으로 분석합니다.

---

## 🧩 신경망 계산의 핵심 개념

### ✅ 벡터화(Vectorization): 반복문에서 행렬 연산으로

기존에는 훈련 데이터를 처리할 때 다음과 같이 for문을 사용하여 샘플을 하나씩 처리하는 방식이 일반적이었습니다.

```python
for i in range(m):
    z[i] = np.dot(w.T, x[i]) + b
```

하지만 신경망에서는 아래와 같이 **모든 샘플을 한꺼번에 처리**하는 **벡터화된 연산**이 필수입니다.

```python
Z = np.dot(w.T, X) + b
```

이러한 벡터화는 단순한 문법상의 변화가 아니라, **병렬 계산 최적화**, **GPU 활용**, **학습 속도 개선**을 위해 매우 중요합니다.

---

### ✅ 정방향 전파와 역방향 전파

신경망 학습은 다음 두 단계로 구성됩니다.

* **정방향 전파(Forward Propagation)**: 입력 \$x\$로부터 예측값 \$\hat{y}\$를 계산
* **역방향 전파(Backward Propagation)**: 예측값과 실제값의 오차를 기반으로 손실 함수의 **기울기(gradient)** 를 계산하고, 이를 통해 가중치 업데이트

로지스틱 회귀는 **단일 뉴런(single neuron)** 기반의 신경망으로 간주할 수 있어, 이 과정을 이해하기 위한 이상적인 출발점입니다.

---

## 📌 문제 정의: 이진 분류(Binary Classification)

이진 분류는 타깃 값 \$y \in {0, 1}\$ 인 문제로, 대표적인 예시는 다음과 같습니다.

* 이메일이 **스팸인지 아닌지**
* 이미지가 **고양이인지 아닌지**
* 종양이 **악성인지 양성인지**

출력값 \$\hat{y}\$는 **확률값**이며, 0 또는 1로 임계 판단(thresholding)하여 최종 분류합니다.

---

## 🔢 입력 데이터 표현: 이미지의 벡터화

예를 들어, \$64 \times 64\$ 크기의 RGB 이미지가 있다면:

* 각 픽셀은 R, G, B 세 개의 숫자로 표현
* 따라서, 하나의 이미지는 \$64 \cdot 64 \cdot 3 = 12,288\$ 개의 실수값으로 표현 가능

이를 하나의 열 벡터로 평탄화하면:

$$
x \in \mathbb{R}^{n_x}, \quad n_x = 12288
$$

\$m\$개의 샘플을 모으면 다음과 같은 입력 행렬이 됩니다:

$$
X = [x^{(1)}, x^{(2)}, ..., x^{(m)}] \in \mathbb{R}^{n_x \times m}
$$

그리고 정답 레이블:

$$
Y = [y^{(1)}, y^{(2)}, ..., y^{(m)}] \in \mathbb{R}^{1 \times m}
$$

> 🔍 **참고**: 샘플을 **열(column)** 기준으로 배치하는 것이 신경망 연산(특히 벡터화)에서 효율적입니다. PyTorch, TensorFlow 등에서도 이 형식을 따릅니다.

---

## 📑 주요 기호 정리

| 기호                                   | 의미                   |
| ------------------------------------ | -------------------- |
| \$n\_x\$                             | 입력 특성의 수 (예: 12,288) |
| \$m\$                                | 훈련 샘플 수              |
| \$X \in \mathbb{R}^{n\_x \times m}\$ | 입력 행렬                |
| \$Y \in \mathbb{R}^{1 \times m}\$    | 출력 레이블 행렬            |
| \$w \in \mathbb{R}^{n\_x}\$          | 가중치 벡터               |
| \$b \in \mathbb{R}\$                 | 편향 (bias)            |

---

## 🧮 정방향 전파 (Forward Propagation)

로지스틱 회귀는 입력 벡터에 대해 다음과 같은 순서를 거칩니다.

### 1단계: 선형 조합 (Linear Combination)

$$
z^{(i)} = w^\top x^{(i)} + b
$$

전체 샘플에 대해 동시에 계산:

$$
Z = w^\top X + b \in \mathbb{R}^{1 \times m}
$$

### 2단계: 시그모이드 활성화 함수

$$
A = \sigma(Z) = \frac{1}{1 + e^{-Z}} \in \mathbb{R}^{1 \times m}
$$

\$A\$는 각 샘플에 대한 **예측 확률**을 담고 있으며, 이 값을 기준으로 분류 여부를 결정합니다.

---

## 📉 비용 함수 (Loss Function): 이진 교차 엔트로피

모델이 예측한 확률 \$\hat{y}^{(i)}\$ 와 실제값 \$y^{(i)}\$ 사이의 오차를 측정하기 위해 **이진 교차 엔트로피(Binary Cross-Entropy)** 를 사용합니다.

### 개별 샘플에 대한 손실:

$$
\mathcal{L}^{(i)} = -\left[y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)}) \right]
$$

### 전체 비용 함수:

$$
J(w, b) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}^{(i)}
$$

이 비용 함수를 최소화하기 위해 **경사 하강법(Gradient Descent)** 을 적용하며, 다음 글에서 **역전파와 기울기 계산**을 다룰 예정입니다.

---

## 🧾 행렬 차원 요약

| 기호                   | 의미     | 차원            |
| -------------------- | ------ | ------------- |
| \$X\$                | 입력 행렬  | \$(n\_x, m)\$ |
| \$w\$                | 가중치 벡터 | \$(n\_x, 1)\$ |
| \$b\$                | 편향     | 스칼라           |
| \$Z = w^\top X + b\$ | 선형 출력  | \$(1, m)\$    |
| \$A = \sigma(Z)\$    | 예측값    | \$(1, m)\$    |
| \$Y\$                | 실제값    | \$(1, m)\$    |
| \$J\$                | 비용     | 스칼라           |

---

## 📊 시각화 예제: 차원 비교

```python
import numpy as np
import matplotlib.pyplot as plt

n_x = 12288  # 특성 수
m = 5        # 샘플 수

plt.figure(figsize=(6, 4))
plt.bar(['특성 수 (n_x)', '샘플 수 (m)'], [n_x, m], color=['blue', 'orange'])
plt.title("입력 행렬 X의 차원 구조")
plt.ylabel("크기")
plt.show()
```

이를 통해 이미지 처리에서 한 샘플의 정보량이 얼마나 큰지를 직관적으로 이해할 수 있습니다.

---

## ✅ 요약 정리

* **로지스틱 회귀**는 시그모이드 함수를 통해 확률 출력을 생성하는 **이진 분류 모델**이다.
* 신경망에서는 **벡터화된 연산**이 계산 효율성과 구현 측면에서 필수적이다.
* 입력 이미지 데이터를 **벡터로 평탄화**하고, 전체 훈련 데이터를 **행렬 X**로 구성한다.
* 예측 확률은 \$\sigma(w^\top X + b)\$ 로 계산되며, 손실은 **이진 교차 엔트로피**로 측정된다.

---

## 🔜 다음 글 예고

다음 글에서는 다음 주제를 다룰 예정입니다.

* **역방향 전파(Backpropagation)** 를 통한 비용 함수의 기울기 계산
* **\$\frac{\partial J}{\partial w}, \frac{\partial J}{\partial b}\$ 의 도출 및 해석**
* **NumPy로 직접 구현한 로지스틱 회귀 학습 코드**

이론적 이해와 구현 능력을 함께 갖춘, 더욱 심화된 내용으로 이어집니다. 🚀






# 📌 Notation 정리 (표기법 요약)

## 1. 개별 샘플 표기

* 입력 벡터 \$x\$:

  $x \in \mathbb{R}^{n_x}$
* 정답 레이블 \$y\$:

  $y\in\{0, 1\}$

## 2. 훈련 샘플 전체

* 총 \$m\$개의 훈련 샘플:

  $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})\}$
* \$M = m\_{\text{train}}\$: 훈련 데이터의 개수
* \$m\_{\text{test}}\$: 테스트 데이터의 개수

---

## 3. 입력 행렬 \$X\$

입력 벡터들을 하나의 행렬로 구성:

$$
X =
\begin{bmatrix}
\vert & \vert & & \vert \\
x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\
\vert & \vert & & \vert \\
\end{bmatrix}
\in \mathbb{R}^{n_x \times m}
$$

* \$n\_x\$: 입력 특성(feature)의 수
* \$m\$: 샘플 수 (열의 수)
* 행렬 차원:

  $$
  X.shape = (n_x, m)
  $$

---

## 4. 출력 벡터 \$Y\$

정답 레이블들을 행렬로 표현:

$$
Y = \begin{bmatrix}
y^{(1)} & y^{(2)} & \cdots & y^{(m)}
\end{bmatrix}
\in \mathbb{R}^{1 \times m}
$$

* 각 \$y^{(i)} \in {0, 1}\$
* 행렬 차원:

$Y.shape = (1, m)$

---
 
## 5. 요약

| 항목          | 차원 (shape)    |
| ----------- | ------------- |
| 입력 행렬 \$X\$ | \$(n\_x, m)\$ |
| 출력 행렬 \$Y\$ | \$(1, m)\$    |

---

