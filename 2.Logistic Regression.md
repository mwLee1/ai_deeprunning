

# 🧠 Binary Classification을 위한 Logistic Regression 이론 정리

## 1. 🚀 서론: 왜 로지스틱 회귀인가?

**로지스틱 회귀(Logistic Regression)** 는 고전적인 통계 기법이면서도 오늘날 **이진 분류(Binary Classification)** 문제에서 여전히 강력하게 활용되는 지도 학습(Supervised Learning) 모델입니다.

이 모델은 입력 벡터 \$x \in \mathbb{R}^{n}\$ 와 학습 파라미터 \$w \in \mathbb{R}^{n},\ b \in \mathbb{R}\$ 를 통해 **\$y \in {0, 1}\$** 값을 예측하며, 출력값은 **\$y = 1\$일 확률**로 해석됩니다.

> 예시: 이미지가 고양이인지 아닌지를 판단하는 문제
>
> * \$y = 1\$: 고양이
> * \$y = 0\$: 고양이 아님

---

## 2. 📐 모델 정의: 선형 결정 함수와 시그모이드

### 2.1 선형 결정 함수 (Linear Decision Function)

로지스틱 회귀는 먼저 선형 모델을 기반으로 **결정 함수 \$z\$** 를 정의합니다:

$$
z = w^T x + b
$$

이 식은 선형 회귀와 구조상 동일하지만, **로지스틱 회귀에서는 \$z\$ 자체를 예측으로 사용하지 않습니다.** 이유는 \$z \in (-\infty, +\infty)\$ 범위를 가지므로 확률 해석이 불가능하기 때문입니다.

---

### 2.2 확률 매핑: 시그모이드 함수 (Sigmoid Function)

결정 함수 \$z\$를 **확률값 $\[0, 1]\$로 매핑**하기 위해 다음과 같은 **시그모이드 함수**를 적용합니다:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

> 이로써 \$\hat{y} = P(y = 1 \mid x) = \sigma(w^T x + b)\$라는 **조건부 확률 모델**이 완성됩니다.

---

## 3. 📊 시그모이드 함수의 특성

| 특성  | 수식 및 해석                                                                          |
| --- | -------------------------------------------------------------------------------- |
| 도메인 | \$z \in \mathbb{R}\$                                                             |
| 범위  | \$\sigma(z) \in (0, 1)\$                                                         |
| 경계값 | \$\sigma(0) = 0.5\$                                                              |
| 극한값 | \$\lim\_{z \to -\infty} \sigma(z) = 0\$, \$\lim\_{z \to +\infty} \sigma(z) = 1\$ |
| 미분  | \$\sigma'(z) = \sigma(z)(1 - \sigma(z))\$ (역전파 시 사용)                             |

### 3.1 시각화

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-10, 10, 100)
y = sigmoid(z)

plt.plot(z, y, label='Sigmoid Function')
plt.axhline(y=0.5, color='r', linestyle='--', label='Decision Boundary ($z=0$)')
plt.xlabel("$z$")
plt.ylabel("$\sigma(z)$")
plt.title("Sigmoid Function for Logistic Regression")
plt.legend()
plt.grid(True)
plt.show()
```

🔍 위 그래프는 로지스틱 회귀가 어떻게 **결정 경계(\$z = 0\$)** 를 기준으로 **확률적 판단을 수행**하는지를 보여줍니다.

---

## 4. 🧠 이진 분류기에서의 확률적 해석

로지스틱 회귀는 결정 경계를 **확률 공간 상에서 정의**합니다:

$$
\hat{y} = \sigma(w^T x + b) = P(y=1 \mid x)
$$

결정 규칙은 일반적으로 **0.5 임계값(threshold)** 를 기준으로 분류합니다:

$$
\hat{y} \geq 0.5 \Rightarrow \text{Class 1 (Positive)}, \quad \hat{y} < 0.5 \Rightarrow \text{Class 0 (Negative)}
$$

---

## 5. 🔩 파라미터 구성 및 표현

로지스틱 회귀의 학습 대상은 다음과 같은 파라미터입니다:

* \$w \in \mathbb{R}^{n}\$: **가중치 벡터** — 각 특성(feature)이 결과에 미치는 영향력
* \$b \in \mathbb{R}\$: **바이어스** — 결정 경계를 이동시키는 역할

이들을 하나의 벡터로 통합하면 다음과 같이 표현할 수 있습니다:

$$
\Theta = 
\begin{bmatrix}
b \\
w_1 \\
w_2 \\
\vdots \\
w_{n}
\end{bmatrix}
$$

> 그러나 실제 구현에서는 \$w\$와 \$b\$를 분리해 두는 것이 **직관적이고 계산적으로 유리**합니다 (예: 그래디언트 분리 계산, 정규화 적용 등).

---

## 6. 📘 통계적 기반: 로지스틱 회귀의 유도

로지스틱 회귀는 **베르누이 확률 분포**를 기반으로 한 **확률적 모델**입니다.

* 조건부 확률 분포:

$$
P(y \mid x; w, b) = \hat{y}^y (1 - \hat{y})^{1 - y}
$$

* 이를 최대화하는 **MLE(Maximum Likelihood Estimation)** 을 적용하면 **Binary Cross-Entropy 손실 함수**가 도출됩니다.

$$
\mathcal{L}(w, b) = \prod_{i=1}^m \hat{y}^{(i)y^{(i)}} (1 - \hat{y}^{(i)})^{1 - y^{(i)}}
\Rightarrow \log \mathcal{L}(w, b) = \sum_{i=1}^{m} \left[y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)})\right]
$$

* 이 로그 가능도 함수의 **음수(-)** 가 바로 비용 함수가 됩니다.
  👉 이는 다음 글에서 자세히 설명합니다.

---

## ✅ 요약 정리

| 항목           | 내용                                          |
| ------------ | ------------------------------------------- |
| **모델 목적**    | \$P(y = 1 \mid x)\$의 조건부 확률 추정              |
| **결정 함수**    | \$z = w^T x + b\$                           |
| **확률 출력**    | \$\hat{y} = \sigma(z)\$                     |
| **분류 기준**    | \$\hat{y} \geq 0.5\$이면 Class 1              |
| **시그모이드 특성** | 비선형 함수, $\[0,1]\$ 범위, 경계: \$\sigma(0)=0.5\$ |
| **파라미터**     | \$w\$ (가중치), \$b\$ (편향), 통합 시 \$\Theta\$    |

---

## 📌 다음 글 예고

다음 글에서는 **로지스틱 회귀의 비용 함수(Cost Function)** 를 정의하고, 이를 **경사 하강법(Gradient Descent)** 으로 최적화하는 과정을 상세히 다룹니다.

이론과 수식을 기반으로 실제 구현과 연동하여 **머신러닝 모델 학습의 수학적 기반**을 명확히 이해할 수 있도록 안내하겠습니다.

---

